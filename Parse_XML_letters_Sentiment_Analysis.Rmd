---
title: "Van Gogh letters"
author: "Theresa Plougstrup"
date: "2025-05-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Parsing xml letters

```{r}
library(tidyverse)
library(xml2)
library(dplyr)
```
```{r}

letters <- tibble(file = list.files("data/Letters", pattern = "let", full.names = TRUE))

```

```{r}
letters <- letters %>% 
  rowwise() %>% 
  mutate(xml = list(read_xml(file))) %>% 
  ungroup()
```
#For one file

```{r}
etBrev <- read_xml("data/Letters/let001.xml")
```

```{r}
etBrev %>%
   xml_find_all("/d1:TEI/d1:teiHeader/d1:fileDesc/d1:sourceDesc/vg:letDesc/vg:letHeading/vg:addressee") %>% 
          xml_text()
  
```

#For all 903 files

I start by opening one of the letter xml files in the program: Visual Studio Code. Here I follow the structure of one of the XML files and write down the path to e.g. the translated, english content of the letter to create a column containing that information.This way the content of all the xml files with letters - as they all have the same structure (which can be tested by seeing if all 903 letters in this case are regisered in the environment window) and the letters are thus usable in Rstudio and I can analyse them further.

For the following analysis I will be doing with these files, I decided to parsing the title of the letter, the English content, the author and the recipient of a letter and the date and place where it was sent from. It is, of course, possible to extract other information from the letters if needed. 

```{r}
#Parsing all xml files

letters_parsed <- letters %>% 
  rowwise() %>% 
  mutate(title = xml %>% 
           xml_find_all("/d1:TEI/d1:teiHeader/d1:fileDesc/d1:titleStmt/d1:title") %>% 
           xml_text(),
         content = xml %>% 
           xml_find_all('/d1:TEI/d1:text/d1:body/d1:div[@type = "translation"]') %>% 
           xml_text(),
        author = xml %>% 
           xml_find_all("/d1:TEI/d1:teiHeader/d1:fileDesc/d1:sourceDesc/vg:letDesc/vg:letHeading/d1:author") %>% 
           xml_text(), 
        recipient = xml %>% 
          xml_find_all("/d1:TEI/d1:teiHeader/d1:fileDesc/d1:sourceDesc/vg:letDesc/vg:letHeading/vg:addressee") %>% 
          xml_text(),
        date = xml %>% 
          xml_find_all("/d1:TEI/d1:teiHeader/d1:fileDesc/d1:sourceDesc/vg:letDesc/vg:letHeading/vg:dateLet") %>% 
          xml_text(), 
        place = xml %>% 
          xml_find_all("/d1:TEI/d1:teiHeader/d1:fileDesc/d1:sourceDesc/vg:letDesc/vg:letHeading/vg:placeLet") %>% 
          xml_text())
```

```{r}
library("tidyverse")
library("ggwordcloud")
library("textdata")
library("tidytext")
library("lubridate")
library("here")
```


```{r}
letters_df <- letters_parsed %>% 
  mutate(text_lines = str_split(content, pattern = '\n')) %>% 
  unnest(text_lines) %>% 
  mutate(text_lines = str_replace_all(text_lines, "’", "'"),
         text_lines = str_trim(str_to_lower(text_lines)))
```

```{r}
letters_tokens <- letters_df %>% 
  unnest_tokens(word, text_lines)
```

```{r}
stop_words <- stop_words %>%
  mutate(
    word = str_replace_all(word, "’", "'"),
    word = str_trim(str_to_lower(word))
  )
```

```{r}
letters_stop <- letters_tokens %>% 
  anti_join(stop_words, by = "word")
```

```{r}
letters_no_numeric <- letters_stop %>% 
  filter(is.na(as.numeric(word)))
```
```{r}
letters_top100 <- letters_no_numeric %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 100)
letters_top100
```
```{r}
view(letters_top100)
```


```{r}
#Filter letters from June 1889 where Starry Night was painted

letters_df <- letters_df %>%
  mutate(
    date_first = str_extract(date, "\\b\\w+,\\s*\\d{1,2}\\s+\\w+\\s+\\d{4}\\b"),
    date_parsed = dmy(str_remove(date_first, "^\\w+,\\s*"))
  )

```

```{r}
letters_june_1889 <- letters_df %>%
  filter(!is.na(date_parsed),
         year(date_parsed) == 1889,
         month(date_parsed) == 6) 
```

```{r}
get_sentiments(lexicon = "afinn")
get_sentiments(lexicon = "nrc")
```
```{r}
june_tokens <- letters_june_1889 %>%
  unnest_tokens(word, text_lines)
```

```{r}
letters_1889_afinn <- june_tokens %>% 
  inner_join(get_sentiments("afinn"), by = "word")

letters_1889_afinn
```
```{r}
letters_afinn_hist <- letters_1889_afinn %>% 
  count(value)

ggplot(data = letters_afinn_hist, aes(x = value, y = n)) +
  geom_col(aes(fill = value)) +
  theme_bw()
```
```{r}
letters_1889_afinn %>%
  summarise(
    mean_score = mean(value),
    median_score = median(value))
```
```{r}
letters_1889_afinn2 <- letters_1889_afinn %>% 
  filter(value == 2)

letters_1889_afinn2
```
```{r}
letters_1889_nrc <- june_tokens %>% 
  inner_join(get_sentiments("nrc"), by = "word")

letters_1889_nrc

```

```{r}
letters_exclude <- june_tokens %>% 
  anti_join(get_sentiments("nrc"))

letters_exclude
```
```{r}
letters_exclude_n <- letters_exclude %>% 
  count(word, sort = TRUE)

head(letters_exclude_n)

```
```{r}
letters_1889_nrc_n <- letters_1889_nrc %>% 
  count(sentiment, sort = TRUE)

ggplot(data = letters_1889_nrc_n, aes(x = sentiment, y = n)) +
  geom_col(aes(fill = sentiment))+
  theme_bw()
```

```{r fig.width=20}
letters_1889_nrc_n5 <- letters_1889_nrc %>%  
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(7) %>% 
  ungroup()

letters_1889_nrc_gg <- ggplot(data = letters_1889_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

letters_1889_nrc_gg
```
```{r}
ggsave(plot = letters_1889_nrc_gg, 
       here("figures","letters_1889_nrc_sentiment.png"), 
       height = 10, 
       width = 7)
```

